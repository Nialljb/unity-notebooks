{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7688a695-825a-4a51-95f0-13a3ba8346a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sync subject and session IDs on Flywheel with local dataframe\n",
    "Niall Bourke  \n",
    "14-11-24   \n",
    "\n",
    "Sometimes the exact IDs may vary between sources due to typos, or variation in use of '_' or '-' or ' '\n",
    "\n",
    "Where possible these should be aligned to assist with future analysis. The purpose of the custom-information-sync gear is to pull a list of subject_ids and session_ids for a project EXACTLY as they are on FW. Taking this list you can sync another source of data ensuring they have the IDs expected on FW. This way when the new data is uploaded it will make sure it goes to the correct matching subject & session. \n",
    "\n",
    "**pre-requisit**\n",
    "Run the 'custom-information-sync' gear\n",
    "\n",
    "**Usage**\n",
    "Go through the following steps:\n",
    "- Look for a recent run of custom-information-sync  \n",
    "- Download output of this gear to the local Jupyter environment   \n",
    "- Upload dataframe to sync to Jupyter environment by selecting upload button  \n",
    "- Run ID syncing cell (this formats the new variables to have the expected subject & session IDs)  \n",
    "- QC the updated file  \n",
    "- Rerun 'custom-information-sync' with this new file as input to sync the info to the matching sessions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4adc8e9e-ba7f-4b02-92f6-a92feeb7cef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: fuzzywuzzy\n",
      "Successfully installed fuzzywuzzy-0.18.0\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
      "Collecting Levenshtein==0.26.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.26.1 python-Levenshtein-0.26.1 rapidfuzz-3.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip install fuzzywuzzy\n",
    "! pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac837a8b-118f-4677-b408-700a453ba85d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# from fuzzywuzzy import process\n",
    "\n",
    "home=os.getcwd()\n",
    "print(os.getcwd())\n",
    "data_path = Path('Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70759bc5-fa5a-4f41-8367-b49bf111dcf7",
   "metadata": {},
   "source": [
    "### Step 2: Selecting and Downloading the Latest Gear Run Output in Flywheel\n",
    "\n",
    "This script retrieves the output of the latest (or second latest) run of a specified Flywheel \"gear\" (processing tool), making it available for local analysis within a Jupyter environment. Here’s an overview of the process:\n",
    "\n",
    "1. **Set Project and Analysis Context**: The code starts by defining the project and retrieving all analyses associated with it.\n",
    "\n",
    "2. **Define Gear Name**: The specific gear name (in this case, `custom-information-sync`) is set to filter analyses. This ensures that only runs using this gear are considered.\n",
    "\n",
    "3. **Filter Analyses by Gear Name**: The script filters through all analyses to find those that match the specified gear name. This filtering is done in a case-insensitive manner.\n",
    "\n",
    "4. **Retrieve Latest Gear Runs**: From the filtered list, it identifies the two most recent gear runs:\n",
    "   - **Latest Gear Run**: The most recent analysis run that matches the specified gear.\n",
    "   - **Second Latest Gear Run**: The second most recent run, used as a backup if the latest run is missing the required files.\n",
    "\n",
    "5. **Check for Files in the Latest Gear Run**: The script checks if the latest run has an output file:\n",
    "   - If the file is found, it proceeds with the latest run.\n",
    "   - If no file is found, it defaults to the second most recent run and logs a message.\n",
    "\n",
    "6. **Download the File Locally**: Once the appropriate file object is identified, it downloads the file to a specified directory within the Jupyter environment, ready for further processing.\n",
    "\n",
    "This script ensures that the most recent data from the specified gear run is readily accessible, with a fallback to a previous run if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e210694c-f35c-4360-9f4c-98b0b29ca449",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'compute_provider_id': None,\n",
      " 'copy_of': None,\n",
      " 'created': datetime.datetime(2024, 11, 13, 14, 4, 32, 803000, tzinfo=tzlocal()),\n",
      " 'description': '',\n",
      " 'files': [],\n",
      " 'gear_info': {'category': 'analysis',\n",
      "               'id': '66df259a61319157aa4ab4d0',\n",
      "               'name': 'custom-information-sync',\n",
      "               'version': '0.0.6'},\n",
      " 'id': '6734b1f053ec76cf0b12b1cf',\n",
      " 'info': {},\n",
      " 'inputs': [{'classification': {},\n",
      "             'client_hash': None,\n",
      "             'copy_of': None,\n",
      "             'created': datetime.datetime(2024, 11, 13, 14, 4, 32, 817000, tzinfo=tzlocal()),\n",
      "             'deid_log_id': None,\n",
      "             'deid_log_skip_reason': None,\n",
      "             'deleted': None,\n",
      "             'file_id': '6734afd94636342723a745cd',\n",
      "             'gear_info': {'id': '66df259a61319157aa4ab4d0',\n",
      "                           'name': 'custom-information-sync',\n",
      "                           'version': '0.0.6'},\n",
      "             'hash': None,\n",
      "             'id': 'ebb1b293-7199-4cb4-8830-0616b8f7785a',\n",
      "             'info': {},\n",
      "             'info_exists': False,\n",
      "             'mimetype': 'text/csv',\n",
      "             'modality': None,\n",
      "             'modified': datetime.datetime(2024, 11, 13, 14, 4, 32, 817000, tzinfo=tzlocal()),\n",
      "             'name': 'sync.csv',\n",
      "             'origin': {'id': 'nialljbourke@gmail.com', 'type': 'user'},\n",
      "             'original_copy_of': None,\n",
      "             'parent_ref': {'id': '6734b1f053ec76cf0b12b1cf',\n",
      "                            'type': 'analysis'},\n",
      "             'parents': {'acquisition': None,\n",
      "                         'analysis': None,\n",
      "                         'group': 'dev',\n",
      "                         'project': '64834ac0807af4cddffe5e2f',\n",
      "                         'session': None,\n",
      "                         'subject': None},\n",
      "             'path': 'ebb1b293-7199-4cb4-8830-0616b8f7785a',\n",
      "             'provider_id': '6260769cce482b781a066fe5',\n",
      "             'reference': None,\n",
      "             'replaced': None,\n",
      "             'restored_by': None,\n",
      "             'restored_from': None,\n",
      "             'size': 135,\n",
      "             'tags': [],\n",
      "             'type': 'tabular data',\n",
      "             'version': 3,\n",
      "             'virus_scan': None,\n",
      "             'zip_member_count': None}],\n",
      " 'job': {'attempt': 1,\n",
      "         'compute_provider_id': '5f071f2c61ebf8000ec33404',\n",
      "         'config': {'config': {'debug': False, 'dry_run': False},\n",
      "                    'destination': {'id': '6734b1f053ec76cf0b12b1cf',\n",
      "                                    'type': 'analysis'},\n",
      "                    'inputs': {'session_info': {'base': 'file',\n",
      "                                                'hierarchy': {'id': '64834ac0807af4cddffe5e2f',\n",
      "                                                              'type': 'project'},\n",
      "                                                'location': {'name': 'sync.csv',\n",
      "                                                             'path': '/flywheel/v0/input/session_info/sync.csv'},\n",
      "                                                'object': {'classification': {},\n",
      "                                                           'file_id': '6734afd94636342723a745cd',\n",
      "                                                           'info': {},\n",
      "                                                           'mimetype': 'text/csv',\n",
      "                                                           'modality': None,\n",
      "                                                           'origin': {'id': 'nialljbourke@gmail.com',\n",
      "                                                                      'type': 'user'},\n",
      "                                                           'size': 135,\n",
      "                                                           'tags': [],\n",
      "                                                           'type': 'tabular '\n",
      "                                                                   'data',\n",
      "                                                           'version': 3,\n",
      "                                                           'zip_member_count': None}}}},\n",
      "         'created': datetime.datetime(2024, 11, 13, 14, 4, 32, 866000, tzinfo=tzlocal()),\n",
      "         'destination': {'id': '6734b1f053ec76cf0b12b1cf', 'type': 'analysis'},\n",
      "         'failure_reason': None,\n",
      "         'gear_id': '66df259a61319157aa4ab4d0',\n",
      "         'gear_info': {'category': 'analysis',\n",
      "                       'id': '66df259a61319157aa4ab4d0',\n",
      "                       'name': 'custom-information-sync',\n",
      "                       'version': '0.0.6'},\n",
      "         'group': None,\n",
      "         'id': '6734b1f053ec76cf0b12b1d1',\n",
      "         'inputs': {'session_info': {'id': '64834ac0807af4cddffe5e2f',\n",
      "                                     'name': 'sync.csv',\n",
      "                                     'type': 'project'}},\n",
      "         'label': '',\n",
      "         'modified': datetime.datetime(2024, 11, 13, 14, 8, 19, 65000, tzinfo=tzlocal()),\n",
      "         'origin': {'id': 'nialljbourke@gmail.com', 'type': 'user'},\n",
      "         'previous_job_id': None,\n",
      "         'priority': 'medium',\n",
      "         'profile': {'elapsed_time_ms': 1500,\n",
      "                     'executor': {'cpu_cores': 4,\n",
      "                                  'disk_bytes': 207929917440,\n",
      "                                  'gpu': None,\n",
      "                                  'host': None,\n",
      "                                  'instance_type': None,\n",
      "                                  'memory_bytes': 33165938688,\n",
      "                                  'name': 'analysis-4792accd',\n",
      "                                  'swap_bytes': None},\n",
      "                     'preparation_time_ms': None,\n",
      "                     'total_input_files': 1,\n",
      "                     'total_input_size_bytes': 135,\n",
      "                     'total_output_files': 0,\n",
      "                     'total_output_size_bytes': 0,\n",
      "                     'total_time_ms': 15110,\n",
      "                     'upload_time_ms': None,\n",
      "                     'versions': None},\n",
      "         'project': None,\n",
      "         'related_container_ids': ['dev',\n",
      "                                   '6734b1f053ec76cf0b12b1cf',\n",
      "                                   '64834ac0807af4cddffe5e2f'],\n",
      "         'request': {'inputs': [{'location': '/',\n",
      "                                 'type': 'docker',\n",
      "                                 'uri': 'bmgf.flywheel.io/custom-information-sync@sha256:fc7a234ddfaae00fb6af4a847e991752d03732e8c4eb26b45ab78f60fdde6d1c',\n",
      "                                 'vu': None},\n",
      "                                {'location': '/flywheel/v0',\n",
      "                                 'type': 'scitran',\n",
      "                                 'uri': '/jobs/6734b1f053ec76cf0b12b1d1/config.json',\n",
      "                                 'vu': None},\n",
      "                                {'location': '/flywheel/v0/input/session_info',\n",
      "                                 'type': 'scitran',\n",
      "                                 'uri': '/files/6734afd94636342723a745cd/version/3/download/sync.csv',\n",
      "                                 'vu': None}],\n",
      "                     'outputs': [{'location': '/flywheel/v0/output',\n",
      "                                  'type': 'scitran',\n",
      "                                  'uri': '/engine?level=analysis&id=6734b1f053ec76cf0b12b1cf&job=6734b1f053ec76cf0b12b1d1',\n",
      "                                  'vu': None}],\n",
      "                     'target': {'command': ['bash', '-c', 'python run.py'],\n",
      "                                'dir': '/flywheel/v0',\n",
      "                                'env': {'BUILD_TIME': '2023-10-30T13:12:10Z',\n",
      "                                        'COMMIT_REF': 'main',\n",
      "                                        'COMMIT_SHA': '771cdc04',\n",
      "                                        'DEBIAN_FRONTEND': 'noninteractive',\n",
      "                                        'EDITOR': 'micro',\n",
      "                                        'FLYWHEEL': '/flywheel/v0',\n",
      "                                        'FW_CONFIG_DEBUG': 'false',\n",
      "                                        'FW_CONFIG_DRY_RUN': 'false',\n",
      "                                        'FW_GID': '31337',\n",
      "                                        'FW_UID': '31337',\n",
      "                                        'GPG_KEY': 'A035C8C19219BA821ECEA86B64E628F8D684696D',\n",
      "                                        'LANG': 'C.UTF-8',\n",
      "                                        'LANGUAGE': 'en_US',\n",
      "                                        'LC_ALL': 'C.UTF-8',\n",
      "                                        'PATH': '/venv/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin',\n",
      "                                        'PIP_DISABLE_PIP_VERSION_CHECK': '1',\n",
      "                                        'PROMETHEUS_MULTIPROC_DIR': '/var/metrics',\n",
      "                                        'PWD': '/flywheel/v0',\n",
      "                                        'PYSITE': '/usr/local/lib/python3.11/site-packages',\n",
      "                                        'PYTHON_GET_PIP_SHA256': '22b849a10f86f5ddf7ce148ca2a31214504ee6c83ef626840fde6e5dcd809d11',\n",
      "                                        'PYTHON_GET_PIP_URL': 'https://github.com/pypa/get-pip/raw/c6add47b0abf67511cdfb4734771cbab403af062/public/get-pip.py',\n",
      "                                        'PYTHON_PIP_VERSION': '23.2.1',\n",
      "                                        'PYTHON_SETUPTOOLS_VERSION': '65.5.1',\n",
      "                                        'PYTHON_VERSION': '3.11.6',\n",
      "                                        'SETUPTOOLS_USE_DISTUTILS': 'stdlib'},\n",
      "                                'gid': None,\n",
      "                                'uid': None}},\n",
      "         'retried': None,\n",
      "         'saved_files': [],\n",
      "         'state': 'complete',\n",
      "         'tags': ['', 'analysis', 'custom-information-sync'],\n",
      "         'transitions': {'cancelled': None,\n",
      "                         'complete': datetime.datetime(2024, 11, 13, 14, 8, 19, 65000, tzinfo=tzlocal()),\n",
      "                         'failed': None,\n",
      "                         'running': datetime.datetime(2024, 11, 13, 14, 8, 3, 955000, tzinfo=tzlocal())}},\n",
      " 'join_origin': None,\n",
      " 'label': 'UPLOAD_13-11-2024_14-04-32',\n",
      " 'modified': datetime.datetime(2024, 11, 13, 14, 4, 32, 899000, tzinfo=tzlocal()),\n",
      " 'notes': [],\n",
      " 'original_copy_of': None,\n",
      " 'parent': {'id': '64834ac0807af4cddffe5e2f', 'type': 'project'},\n",
      " 'parents': {'acquisition': None,\n",
      "             'group': 'dev',\n",
      "             'project': '64834ac0807af4cddffe5e2f',\n",
      "             'session': None,\n",
      "             'subject': None},\n",
      " 'revision': 2,\n",
      " 'tags': [],\n",
      " 'timestamp': None}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "project = fw_project\n",
    "analyses = project.analyses\n",
    "\n",
    "# Specify the gear name to search for\n",
    "gear='custom-information-sync'\n",
    "gear_to_find = gear.strip()  # Assuming 'gear' is the gear name you're looking for\n",
    "\n",
    "# Filter the analyses by the gear name\n",
    "filtered_gear_runs = [\n",
    "    run for run in analyses\n",
    "    if run.get('gear_info', {}).get('name', '').strip().casefold() == gear_to_find.casefold()\n",
    "]\n",
    "\n",
    "# Get the most recent and second most recent gear runs\n",
    "latest_gear_run = filtered_gear_runs[-1]\n",
    "second_latest_gear_run = filtered_gear_runs[-2]\n",
    "\n",
    "# Attempt to get the file object from the latest gear run\n",
    "file_object = latest_gear_run.files\n",
    "\n",
    "# Check if the file_object is None or an empty list\n",
    "if not file_object:  # This will be True if file_object is None or an empty list\n",
    "    # If no file is found in the latest gear run, use the second most recent run\n",
    "    file_object = second_latest_gear_run.files\n",
    "    print(\"Using second most recent run. File object: \", file_object[0].name)\n",
    "else:\n",
    "    print(\"Using latest run. File object: \", file_object[0].name)\n",
    "\n",
    "# Pulling file into Jupyter env\n",
    "download_path = Path(home) / \"Data/\" / file_object[0].name\n",
    "file_object[0].download(download_path)\n",
    "file_str = file_object[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e549c-04a8-47cd-881d-1c2d0f1562a5",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Syncing Local Data with Flywheel Session Information\n",
    "\n",
    "This script combines local demographic or variable data with session data from a Flywheel project, aligning information for seamless synchronization. Here’s an overview of its key steps:\n",
    "\n",
    "1. **Define File Paths**: The script defines file paths for accessing the local data (e.g., a demographics CSV file) and Flywheel session information CSV.\n",
    "\n",
    "2. **Import Data**: It loads the local data (`df_local`) and Flywheel session data (`df_fw`) into separate dataframes. These dataframes contain information that will later be merged, such as participant demographics and session details.\n",
    "\n",
    "3. **Align Identifiers**: The script renames columns in `df_local` to match those in Flywheel (`df_fw`). This alignment ensures that subject IDs and session IDs are consistent, enabling the data to be matched accurately.\n",
    "\n",
    "4. **Merge Dataframes**: The `merge()` function combines `df_fw` and `df_local` based on shared identifiers (e.g., `subject_id`), allowing demographic data to be added to the Flywheel session data.\n",
    "\n",
    "5. **Replace Data Where Needed**: For each relevant column, the script replaces Flywheel values with those from the local data if they are available. This allows updated or additional information from the local dataset to overwrite existing values in Flywheel data.\n",
    "\n",
    "6. **Clean Up Columns**: After merging, the script removes unnecessary columns (suffixes created by merging, like `_fw` and `_local`) to keep the dataframe clean and organized.\n",
    "\n",
    "7. **Save the Merged Dataframe**: Finally, the script saves the merged and cleaned dataframe back to a CSV file, ready for uploading or further analysis.\n",
    "\n",
    "This process enables efficient synchronization of locally stored data with Flywheel, ensuring that session information is accurate and up-to-date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e1723a7f-84f8-4442-9405-5821cf6253af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define file path for local data and Flywheel session information\n",
    "f = Path(Path.home() / 'Data' / 'NEWDATA.csv')\n",
    "\n",
    "# Import local dataframe with demographics or variables you want to sync to Flywheel project\n",
    "df_local = pd.read_csv(f)\n",
    "\n",
    "# Import session information dataframe from Flywheel\n",
    "df_fw = pd.read_csv(Path(Path.home() / \"Data\" / file_object[0].name))\n",
    "\n",
    "# NOTE: Align subject IDs between dataframes. If the IDs don't align, Flywheel will not be able to assign the variables to the correct sessions\n",
    "df_local = df_local.rename(columns={\"studyid\": \"subject_id\", \n",
    "                                    \"age_months_at_scan\": \"age_months\"})\n",
    "\n",
    "# Merge the dataframes on subject_id and session_id\n",
    "merged_df = df_fw.merge(df_local, on=[\"subject_id\"], how=\"left\", suffixes=('_fw', '_local'))\n",
    "\n",
    "# Replace values in df_fw with those from df_local for matching columns\n",
    "columns_to_replace = [col for col in df_local.columns if col not in [\"subject_id\", \"session_id\"]]\n",
    "\n",
    "for column in columns_to_replace:\n",
    "    # If the local data has a value, use it; otherwise, keep the original df_fw value\n",
    "    merged_df[column] = merged_df[column + \"_local\"].combine_first(merged_df[column + \"_fw\"])\n",
    "\n",
    "# Drop the unnecessary '_fw' and '_local' columns created by the merge\n",
    "merged_df.drop(columns=[col + \"_fw\" for col in columns_to_replace] + [col + \"_local\" for col in columns_to_replace], inplace=True)\n",
    "\n",
    "merged_df = merged_df.rename(columns={'session_id_fw': 'session_id'})\n",
    "merged_df = merged_df.drop(columns=['session_id_local'])\n",
    "\n",
    "# Write the merged dataframe back to CSV\n",
    "merged_df.to_csv(file_str, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c8959fa-d5a9-4087-81ef-16456ea7d5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57fb2e87-d00c-4479-a739-ed97e9baac86",
   "metadata": {},
   "source": [
    "# Step 4: Uploading a File and Running a Gear Analysis in Flywheel\n",
    "\n",
    "This final script cell uploads a processed file to Flywheel and initiates a new analysis job using a specified gear. Here’s an overview of the steps:\n",
    "\n",
    "1. **Upload the File to Flywheel**: The script uploads the prepared file (`file_str`) to the Flywheel project. It stores the uploaded file as `input_file`, which serves as an input reference for the gear analysis.\n",
    "\n",
    "2. **Select the Gear**: The `custom-information-sync` gear is retrieved from Flywheel, which will process the uploaded file.\n",
    "\n",
    "3. **Prepare Job Inputs**: The script sets up an `inputs` dictionary where `session_info` is linked to the uploaded file. This dictionary will pass the file as input to the gear.\n",
    "\n",
    "4. **Define Destination and Label for Analysis**: The analysis destination is set to the project level. The script generates a unique `analysis_label` using the current date and time, ensuring each run is clearly labeled.\n",
    "\n",
    "5. **Run the Gear Analysis Job**:\n",
    "   - The script calls the gear’s `run` function, providing the label, inputs, destination, and an empty configuration.\n",
    "   - It stores the job ID in `job_list` and logs a message indicating that the job has been submitted.\n",
    "\n",
    "6. **Handle Errors**: If an error occurs during job submission, the script catches the exception and prints a warning with the specific error message.\n",
    "\n",
    "This code cell completes the workflow by initiating an analysis job in Flywheel, using the uploaded file as input, and tagging it for easy tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a96b4434-c63b-4b30-b307-96c3abc80447",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting Job: Check Jobs Log ISMRM_UNITY_DEMO\n"
     ]
    }
   ],
   "source": [
    "# Get the Flywheel File object using the file path\n",
    "project = fw_project\n",
    "upload_result = project.upload_file(file_str)  # Upload the file to Flywheel and get the file object\n",
    "input_file = upload_result[0]\n",
    "\n",
    "gear =  fw.lookup('gears/custom-information-sync')\n",
    "analysis_tag = 'UPLOAD'\n",
    "job_list = list()     \n",
    "\n",
    "# Initialize the 'inputs' dictionary\n",
    "inputs = {}\n",
    "\n",
    "# Add the file reference to the inputs dictionary\n",
    "inputs['session_info'] = input_file  # The value is now the file reference object\n",
    "\n",
    "    \n",
    "try:\n",
    "    # The destination for this analysis will be on the session\n",
    "    dest = project\n",
    "    analysis_label = f'{analysis_tag}_{formatted_timestamp()}'\n",
    "    job_id = gear.run(\n",
    "        analysis_label=analysis_label,\n",
    "        inputs=inputs,\n",
    "        destination=dest,\n",
    "        tags=[''],\n",
    "        config={}\n",
    "    )\n",
    "    job_list.append(job_id)\n",
    "    print(\"Submitting Job: Check Jobs Log\", dest.label)\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Job cannot be sent for {dest.label}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ce5fc-3dee-4520-aab9-05159e88b604",
   "metadata": {},
   "source": [
    "### Advanced level: refactored as a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3244dc-6e56-42e8-b5a0-260e5277c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_and_run_gear(project, file_path, gear_name, analysis_tag):\n",
    "    # Upload file\n",
    "    upload_result = project.upload_file(file_path)\n",
    "    input_file = upload_result[0] if upload_result else None\n",
    "    \n",
    "    if not input_file:\n",
    "        print(\"Failed to upload file.\")\n",
    "        return\n",
    "\n",
    "    # Set up gear and run job\n",
    "    gear = fw.lookup(f'gears/{gear_name}')\n",
    "    inputs = {'session_info': input_file}\n",
    "    analysis_label = f'{analysis_tag}_{formatted_timestamp()}'\n",
    "    \n",
    "    try:\n",
    "        job_id = gear.run(analysis_label=analysis_label, inputs=inputs, destination=project, tags=[''])\n",
    "        print(\"Job submitted:\", job_id)\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Job could not be sent. Error: {e}\")\n",
    "\n",
    "        \n",
    "upload_and_run_gear(fw_project, file_str, 'custom-information-sync', 'UPLOAD')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d286f636-7a9a-4b1a-a766-987d54d81c4e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BETA ##\n",
    "\n",
    "### Making life complicated by derfining functions\n",
    "- The benifit is to recycle helper fuctions\n",
    "- In theory makes it easier to maintain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf60d0-ac6f-454e-b979-b3159ec33988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def formatted_timestamp():\n",
    "    return datetime.now().strftime('%d-%m-%Y_%H-%M-%S')\n",
    "\n",
    "\n",
    "def get_latest_gear_run(analyses, gear_name):\n",
    "    \"\"\"Returns the latest and second latest runs of the specified gear.\"\"\"\n",
    "    filtered_runs = [\n",
    "        run for run in analyses\n",
    "        if run.get('gear_info', {}).get('name', '').strip().casefold() == gear_name.casefold()\n",
    "    ]\n",
    "    if len(filtered_runs) >= 2:\n",
    "        return filtered_runs[-1], filtered_runs[-2]\n",
    "    elif filtered_runs:\n",
    "        return filtered_runs[-1], None\n",
    "    return None, \n",
    "\n",
    "\n",
    "\n",
    "project = fw_project\n",
    "analyses = project.analyses\n",
    "latest_gear_run, second_latest_gear_run = get_latest_gear_run(analyses, 'custom-information-sync')\n",
    "print(second_latest_gear_run.files)\n",
    "\n",
    "\n",
    "def download_latest_file(run, backup_run, download_dir):\n",
    "    # Check if `run` exists and has files, otherwise fallback to `backup_run`\n",
    "    file_object = run.files if run and run.files else (backup_run.files if backup_run and backup_run.files else None)\n",
    "    \n",
    "    if file_object:\n",
    "        download_path = download_dir / file_object[0].name\n",
    "        file_object[0].download(download_path)\n",
    "        print(f\"Using {'latest' if run and run.files else 'second latest'} run. File: {file_object[0].name}\")\n",
    "        return download_path\n",
    "    \n",
    "    print(\"No file found in specified runs.\")\n",
    "    print(\"Submitting a job to pull subject/session list\")\n",
    "    \n",
    "    gear =  fw.lookup('gears/custom-information-sync')\n",
    "    job_list = list()     \n",
    "    try:\n",
    "        # The destination for this analysis will be on the session\n",
    "        dest = project\n",
    "        analysis_label = f'custom-information-sync_{formatted_timestamp()}'\n",
    "        job_id = gear.run(\n",
    "            analysis_label=analysis_label,\n",
    "            inputs=[],\n",
    "            destination=dest,\n",
    "            tags=[''],\n",
    "            config={}\n",
    "        )\n",
    "        job_list.append(job_id)\n",
    "        print(\"Submitting Job: Check Jobs Log\", dest.label)\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Job cannot be sent for {dest.label}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "download_path = Path(home) / \"Data/\" \n",
    "\n",
    "download_latest_file(latest_gear_run, second_latest_gear_run, download_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
